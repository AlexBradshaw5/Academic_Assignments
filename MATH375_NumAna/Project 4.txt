# -*- coding: utf-8 -*-
"""
Created on Fri Oct 15 13:45:37 2021

@author: alexb
"""

import sklearn as sk
import sklearn.datasets
import numpy as np
import sklearn.model_selection
import timeit

dataset = sk.datasets.fetch_california_housing()

feature_vectors = dataset.data
y = dataset.target
column_names = dataset.feature_names

x = feature_vectors
x = np.insert(x,0,1,axis = 1)
N = x.shape[0]
D = x.shape[1]-1
beta0 = np.zeros(D+1)

x_train,x_val,y_train,y_val = sk.model_selection.train_test_split(x,y,train_size = 0.75, random_state = 123)


MAX_ITER = 100000
h = 1e-7

def L(B,x,y):
    return (1/N)*(np.linalg.norm(x@B-y))**2

def grad(B, x, y):
    inside = (x@B)-y
    return (2/N)*(x.T@inside)
def gradient_descent(beta,x,y):
    for i in range(MAX_ITER):    
        beta = beta - h*grad(beta,x,y)
    return beta


start = timeit.default_timer()
betaEst = gradient_descent(beta0, x_train, y_train)
end = timeit.default_timer()
print("Time of gradient descent is :" ,end-start)
start = timeit.default_timer()
betaTrue = np.linalg.solve(x_train.T @ x_train,x_train.T @ y_train)
end = timeit.default_timer()
print("Time of the normal equation is :" , end-start)

#The normal equation is much quicker than gradient descent since in order to get a precise estimate we must use a large iteration

predictions_gd = x_val@beta0
predictions_norm = x_val@betaTrue

print("L value generated by beta with gd is :", L(betaEst,x_train,y_train))
print("L value generated by beta with normal equation is :", L(betaTrue,x_train,y_train))

#The normal equation provides a much smaller value of L

sme_gd = np.mean((y_val - predictions_gd)**2) #squared mean error gradient descent
sme_py = np.mean((y_val - predictions_norm)**2)

#The normal equation also provides a much smaller squared mean error

print("Squared mean average with beta from gd is :",sme_gd)
print("Squared mean average with beta from norm equation is: ",sme_py)
